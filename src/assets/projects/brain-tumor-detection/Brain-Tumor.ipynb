{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Classification using Convolutional Neural Networks\n",
    "\n",
    "<img src='assets/brain-tumor-banner.png' width='750'>\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook demonstrates the implementation of a CNN-based deep learning model for brain tumor classification using MRI images. The model achieves 94.8% accuracy in classifying four different types of brain conditions.\n",
    "\n",
    "### Important Notes:\n",
    "- **Dataset**: Available on [Kaggle](https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset)\n",
    "- **Training Time**: ~30 minutes\n",
    "- **Requirements**: See `requirements.txt`\n",
    "\n",
    "### Medical Context\n",
    "Brain tumors represent a major challenge in neurology, with over 250,000 new cases diagnosed worldwide each year. Early and accurate diagnosis is crucial for optimizing treatment and improving patient prognosis.\n",
    "\n",
    "### Types of Analyzed Tumors\n",
    "\n",
    "<div style='display: flex; justify-content: space-between;'>\n",
    "    <div style='text-align: center;'>\n",
    "        <img src='assets/Glioma-example.jpg' width='200'>\n",
    "        <p><b>Glioma</b><br>Tumor developed from glial cells</p>\n",
    "    </div>\n",
    "    <div style='text-align: center;'>\n",
    "        <img src='assets/Meningioma-example.jpg' width='200'>\n",
    "        <p><b>Meningioma</b><br>Tumor of the meninges</p>\n",
    "    </div>\n",
    "    <div style='text-align: center;'>\n",
    "        <img src='assets/Pituitary-example.jpg' width='200'>\n",
    "        <p><b>Pituitary Tumor</b><br>Tumor of the pituitary gland</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "### Project Objectives\n",
    "1. Develop an automatic classification model achieving ~95% accuracy\n",
    "2. Create a robust MRI image processing pipeline\n",
    "3. Provide a diagnostic support tool for healthcare professionals\n",
    "\n",
    "### Technologies and Tools\n",
    "- **Deep Learning**: TensorFlow 2.x, Keras\n",
    "- **Image Processing**: OpenCV, PIL\n",
    "- **Data Analysis**: NumPy, Pandas\n",
    "- **Visualization**: Matplotlib, Seaborn\n",
    "\n",
    "### Project Structure\n",
    "```\n",
    "brain_tumor_model_CNN/\n",
    "├── brain_tumor_model_CNN.py    # Main script\n",
    "├── data/                       # Dataset directory\n",
    "│   ├── Training/               # Training images\n",
    "│   └── Testing/                # Test images\n",
    "├── model_performance.csv       # Performance metrics\n",
    "└── visualizations/             # Generated visualizations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "### Required Libraries\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Environment setup\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "# Version information\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tf.keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "\n",
    "These parameters have been optimized through experimentation limited by my harware:\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|--------|-------------|\n",
    "| BATCH_SIZE | 32 | Optimal for memory usage and training speed |\n",
    "| IMAGE_SIZE | 256 | Preserves important medical details |\n",
    "| CHANNELS | 3 | RGB image processing |\n",
    "| EPOCHS | 20 | Prevents overfitting while ensuring convergence |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 256 \n",
    "CHANNELS = 3\n",
    "EPOCHS = 20\n",
    "\n",
    "# Directory paths\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "parent_dir = os.path.dirname(os.path.dirname(current_dir))\n",
    "data_dir = os.path.join(parent_dir, \"data\", \"Training\")\n",
    "test_data_dir = os.path.join(parent_dir, \"data\", \"Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline\n",
    "\n",
    "### 2.1 Data Loading Function\n",
    "\n",
    "The `load_data` function implements a robust data loading pipeline with the following features:\n",
    "- Automatic directory structure parsing\n",
    "- Image resizing and normalization\n",
    "- Batch processing\n",
    "- Shuffling for better training\n",
    "\n",
    "**Note**: Before running this code, ensure you have downloaded and extracted the dataset from Kaggle to the appropriate directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, batch_size, image_size):\n",
    "    \"\"\"Load and preprocess the image dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Path to the data directory\n",
    "        batch_size (int): Number of images per batch\n",
    "        image_size (int): Target size for image resizing\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset: Preprocessed dataset ready for training\n",
    "    \"\"\"\n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        seed=123,\n",
    "        shuffle=True,\n",
    "        image_size=(image_size, image_size),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "# Load datasets\n",
    "# Note: This assumes you have already downloaded and structured the dataset\n",
    "dataset = load_data(data_dir, BATCH_SIZE, IMAGE_SIZE)\n",
    "test_dataset = load_data(test_data_dir, BATCH_SIZE, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Splitting\n",
    "\n",
    "The dataset is split into training and validation sets using the following ratios:\n",
    "- Training: 90%\n",
    "- Validation: 10%\n",
    "\n",
    "This split ensures enough data for training while maintaining a representative validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(ds, train_split=0.9, val_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    \"\"\"Split the dataset into training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        ds (tf.data.Dataset): Input dataset\n",
    "        train_split (float): Proportion for training (default: 0.9)\n",
    "        val_split (float): Proportion for validation (default: 0.1)\n",
    "        shuffle (bool): Whether to shuffle the dataset\n",
    "        shuffle_size (int): Buffer size for shuffling\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_dataset, val_dataset)\n",
    "    \"\"\"\n",
    "    ds_size = len(ds)\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "# Split dataset\n",
    "train_ds, val_ds = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Performance Optimization\n",
    "\n",
    "The following optimizations are implemented to improve training performance:\n",
    "- Data caching to prevent I/O bottlenecks\n",
    "- Prefetching to optimize CPU-GPU pipeline\n",
    "- Memory-efficient shuffling\n",
    "\n",
    "**Note**: These optimizations are crucial for handling the large medical imaging dataset efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization settings\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "# Class names definition\n",
    "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "n_classes = len(class_names)\n",
    "\n",
    "# Apply optimizations to datasets\n",
    "train_ds = train_ds.cache().shuffle(SHUFFLE_BUFFER_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "### 3.1 CNN Architecture Overview\n",
    "\n",
    "\n",
    "```\n",
    "Input Layer (256x256x3)\n",
    "    ↓\n",
    "Rescaling (1/255)\n",
    "    ↓\n",
    "Conv2D (16 filters) + ReLU\n",
    "    ↓\n",
    "MaxPooling2D\n",
    "    ↓\n",
    "Conv2D (32 filters) + ReLU\n",
    "    ↓\n",
    "MaxPooling2D\n",
    "    ↓\n",
    "Conv2D (64 filters) + ReLU\n",
    "    ↓\n",
    "MaxPooling2D\n",
    "    ↓\n",
    "Flatten\n",
    "    ↓\n",
    "Dense (128) + ReLU\n",
    "    ↓\n",
    "Dense (4) + Softmax\n",
    "```\n",
    "\n",
    "**Key Features**:\n",
    "- Progressive filter increase (16→32→64)\n",
    "- Multiple convolution layers for feature extraction\n",
    "- MaxPooling for dimensionality reduction\n",
    "- Dense layers for classification\n",
    "\n",
    "**Note**: This architecture was chosen after extensive experimentation with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Create and compile the CNN model.\n",
    "    \n",
    "    The model architecture is optimized for brain tumor classification,\n",
    "    with multiple convolution layers for feature extraction and\n",
    "    dense layers for classification.\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model ready for training\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input normalization\n",
    "        layers.Rescaling(1./255, input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS)),\n",
    "        \n",
    "        # First convolution block\n",
    "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        \n",
    "        # Second convolution block\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        \n",
    "        # Third convolution block\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        \n",
    "        # Classification layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Model compilation\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "# Note: This cell will only create the model structure, not train it\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "### 4.1 Training Process\n",
    "\n",
    "**Important Note**: The training process takes approximately 30 minutes. This notebook shows the code and expected results, but actual training should be done separately.\n",
    "\n",
    "Training Configuration:\n",
    "- Epochs: 20\n",
    "- Batch Size: 32\n",
    "- Optimizer: Adam\n",
    "- Loss Function: Sparse Categorical Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "# Note: This cell requires significant computational resources\n",
    "# and will take ~30 minutes to complete\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Visualization\n",
    "\n",
    "The following functions visualize the training progress and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Visualize training metrics over time.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history from model.fit()\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "# Note: This requires a trained model's history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result for a generated model :\n",
    "\n",
    "<img src='assets/training_results.png' width='1200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "### 5.1 Performance Metrics\n",
    "\n",
    "The model is evaluated using multiple metrics:\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "- Per-class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(model, test_dataset):\n",
    "    \"\"\"Generate and visualize confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_dataset: Test dataset\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for images, labels in test_dataset:\n",
    "        predictions = model.predict(images)\n",
    "        y_pred.extend(np.argmax(predictions, axis=1))\n",
    "        y_true.extend(labels.numpy())\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, \n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "# Generate confusion matrix\n",
    "# Note: Requires a trained model\n",
    "create_confusion_matrix(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result for a generated model :\n",
    "\n",
    "<img src='assets/confusion_matrix.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prediction Analysis\n",
    "\n",
    "Let's analyze the model's predictions in detail, including:\n",
    "- Correct predictions with confidence levels\n",
    "- Misclassified cases analysis\n",
    "- Confidence distribution\n",
    "\n",
    "**Note**: These visualizations help understand the model's strengths and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_diagrams(model, test_dataset):\n",
    "    \"\"\"Create comprehensive prediction analysis diagrams.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CNN model\n",
    "        test_dataset: Dataset for testing\n",
    "        \n",
    "    Generates:\n",
    "        - Incorrect predictions scatter plot\n",
    "        - Confidence distribution for correct predictions\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    all_confidences = []\n",
    "\n",
    "    for images, labels in test_dataset:\n",
    "        predictions = model.predict(images)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        confidences = np.max(predictions, axis=1)\n",
    "        \n",
    "        all_predictions.extend(predicted_classes)\n",
    "        all_true_labels.extend(labels.numpy())\n",
    "        all_confidences.extend(confidences)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'True Label': [class_names[i] for i in all_true_labels],\n",
    "        'Predicted Label': [class_names[i] for i in all_predictions],\n",
    "        'Confidence': all_confidences\n",
    "    })\n",
    "\n",
    "    df['Correct'] = df['True Label'] == df['Predicted Label']\n",
    "\n",
    "    # Visualization of incorrect predictions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(data=df[~df['Correct']], \n",
    "                    x='True Label', \n",
    "                    y='Predicted Label', \n",
    "                    hue='Confidence', \n",
    "                    size='Confidence', \n",
    "                    sizes=(20, 200),\n",
    "                    palette='viridis')\n",
    "    plt.title('Incorrect Predictions with Confidence Levels')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Confidence distribution for correct predictions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=df[df['Correct']], \n",
    "                x='True Label', \n",
    "                y='Confidence')\n",
    "    plt.title('Confidence Distribution for Correct Predictions')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate prediction analysis\n",
    "# Note: Requires a trained model\n",
    "create_prediction_diagrams(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result for a generated model :\n",
    "\n",
    "<img src='assets/predictions.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Persistence and Deployment\n",
    "\n",
    "### 6.1 Saving the Model\n",
    "\n",
    "The model is saved in two formats:\n",
    "1. Weights (H5 format)\n",
    "2. Architecture (JSON format)\n",
    "\n",
    "This allows for easy model reconstruction and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "model.save_weights(os.path.join(current_dir, \"brain_tumor_model_weights.weights.h5\"))\n",
    "\n",
    "# Save model architecture\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(current_dir, \"brain_tumor_model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "print(\"Model saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Loading and Prediction\n",
    "\n",
    "Example of how to load and use the saved model for predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_predict(image_path):\n",
    "    \"\"\"Load saved model and make predictions.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (predicted_class, confidence)\n",
    "    \"\"\"\n",
    "    # Load model architecture\n",
    "    json_file = open(os.path.join(current_dir, \"brain_tumor_model.json\"), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # Load weights\n",
    "    loaded_model.load_weights(os.path.join(current_dir, \"brain_tumor_model_weights.weights.h5\"))\n",
    "    \n",
    "    # Prepare image\n",
    "    img = tf.keras.preprocessing.image.load_img(\n",
    "        image_path, \n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "    )\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = loaded_model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = float(np.max(predictions[0]))\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Example usage:\n",
    "# predicted_class, confidence = load_and_predict('path/to/your/image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results and Conclusions\n",
    "\n",
    "### 7.1 Performance Summary\n",
    "\n",
    "The model achieved:\n",
    "- Overall Accuracy: 94.8%\n",
    "- Average Prediction Time: < 2 seconds\n",
    "- Consistent performance across all tumor types\n",
    "\n",
    "### 7.2 Key Findings\n",
    "\n",
    "1. **Model Strengths**:\n",
    "   - High accuracy across all classes\n",
    "   - Robust to image variations\n",
    "   - Fast inference time\n",
    "\n",
    "2. **Limitations**:\n",
    "   - Requires high-quality MRI images\n",
    "   - Limited to four specific categories\n",
    "   - No tumor localization\n",
    "\n",
    "### 7.3 Future Improvements\n",
    "\n",
    "1. **Technical Enhancements**:\n",
    "   - Implementation of tumor segmentation\n",
    "   - Support for additional tumor types\n",
    "   - Model optimization for mobile devices\n",
    "\n",
    "2. **Practical Applications**:\n",
    "   - Web interface development\n",
    "   - Integration with medical imaging systems\n",
    "   - Clinical validation studies\n",
    "\n",
    "### 7.4 Usage Instructions\n",
    "\n",
    "To use this model:\n",
    "1. Download the dataset from Kaggle\n",
    "2. Install required dependencies\n",
    "3. Run training script (approximately 30 minutes)\n",
    "4. Use the prediction interface for new images\n",
    "\n",
    "For detailed implementation, refer to the main script: `brain_tumor_model_CNN.py`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
